'use client';

import React, { useState, useCallback, useEffect, useRef } from 'react';
import { startCall, endCall } from '@/lib/callFunctions';
import { CallConfig, SelectedTool } from '@/lib/types';
import { larsWiktoriaEnhancedConfig as demoConfig } from '@/app/lars-wiktoria-enhanced-config';
import { Role, Transcript, UltravoxExperimentalMessageEvent, UltravoxSessionStatus } from 'ultravox-client';
import { saveConversation, updateConversationEnd, saveFullTranscript, Conversation } from '@/lib/supabase';
import VoiceActivation from './VoiceActivation';
// TEMP DISABLED: import PhoneTonePlayer, { PhoneTonePlayerRef } from './PhoneTonePlayer';
import { SimplePhoneTone } from '@/lib/simplePhoneTone';
import { getModeConfig } from '@/lib/exhibitionMode';

interface ExhibitionInterfaceProps {
  /** Optional callback when exhibition session starts */
  onSessionStart?: () => void;
  /** Optional callback when exhibition session ends */
  onSessionEnd?: () => void;
  /** Show development info */
  showDebugInfo?: boolean;
}

export default function ExhibitionInterface({
  onSessionStart,
  onSessionEnd,
  showDebugInfo = false
}: ExhibitionInterfaceProps) {
  // Call state
  const [isCallActive, setIsCallActive] = useState(false);
  const [agentStatus, setAgentStatus] = useState<string>('ready');
  const [callTranscript, setCallTranscript] = useState<Transcript[] | null>([]);
  const [currentVoiceId, setCurrentVoiceId] = useState<string>('876ac038-08f0-4485-8b20-02b42bcf3416');
  const [currentAgent, setCurrentAgent] = useState<string>('lars');
  const [currentConversation, setCurrentConversation] = useState<Conversation | null>(null);
  const [ultravoxCallId, setUltravoxCallId] = useState<string | null>(null);

  // Exhibition state
  const [isWaitingForVoice, setIsWaitingForVoice] = useState(true);
  const [sessionTimeoutId, setSessionTimeoutId] = useState<number | null>(null);
  const [userSilenceTimeoutId, setUserSilenceTimeoutId] = useState<number | null>(null);
  const [lastActivityTime, setLastActivityTime] = useState<number>(Date.now());
  const [lastUserTranscriptTime, setLastUserTranscriptTime] = useState<number>(Date.now());
  const [lastAnySpeechTime, setLastAnySpeechTime] = useState<number>(Date.now()); // Track when anyone (user OR agent) last spoke
  
  // Voice and audio state
  const [voiceActivationEnabled, setVoiceActivationEnabled] = useState(true); // START ENABLED
  const [phoneToneEnabled, setPhoneToneEnabled] = useState(true); // START ENABLED FOR AMBIENT SOUND
  const [hasUserGesture, setHasUserGesture] = useState(false); // Track if we have user gesture
  const [showAudioEnableOverlay, setShowAudioEnableOverlay] = useState(false); // Show click-to-enable overlay
  const [userHasResponded, setUserHasResponded] = useState(false); // Track if user has spoken to Lars yet
  const [conversationEnding, setConversationEnding] = useState(false); // Prevent duplicate end triggers
  
  const transcriptContainerRef = useRef<HTMLDivElement>(null);
  const conversationRef = useRef<Conversation | null>(null);
  // TEMP DISABLED: const phoneToneRef = useRef<PhoneTonePlayerRef>(null);
  const simpleToneRef = useRef<SimplePhoneTone | null>(null);
  
  // Get exhibition configuration
  const modeConfig = getModeConfig();
  const sessionTimeout = modeConfig.autoTimeout || 45000; // 45 seconds default
  const userSilenceTimeout = modeConfig.userSilenceEndCall || 12000; // 12 seconds default
  
  // Debug logging for exhibition interface + LOG CAPTURE
  useEffect(() => {
    // CAPTURE CONSOLE LOGS FOR DEBUGGING
    const originalLog = console.log;
    const originalError = console.error;
    const logBuffer: string[] = [];
    
    console.log = (...args) => {
      const message = args.map(arg => typeof arg === 'object' ? JSON.stringify(arg) : String(arg)).join(' ');
      logBuffer.push(`[LOG] ${new Date().toISOString()}: ${message}`);
      originalLog.apply(console, args);
      
      // Save to localStorage every 10 logs
      if (logBuffer.length % 10 === 0) {
        localStorage.setItem('exhibition_logs', JSON.stringify(logBuffer));
      }
      
      // Also save final logs on key events
      if (message.includes('ðŸ’¾') || message.includes('ðŸ”„') || message.includes('ðŸ”—')) {
        localStorage.setItem('exhibition_logs', JSON.stringify(logBuffer));
      }
    };
    
    console.error = (...args) => {
      const message = args.map(arg => typeof arg === 'object' ? JSON.stringify(arg) : String(arg)).join(' ');
      logBuffer.push(`[ERROR] ${new Date().toISOString()}: ${message}`);
      originalError.apply(console, args);
      localStorage.setItem('exhibition_logs', JSON.stringify(logBuffer));
    };
    
    console.log('ðŸŽ¨ Exhibition interface mounted');
    console.log('ðŸŽ¨ Mode config:', modeConfig);
    console.log('ðŸŽ¨ Session timeout:', sessionTimeout);
    console.log('ðŸŽ¨ User silence timeout:', userSilenceTimeout);
    console.log('ðŸŽ¨ Voice activation enabled:', voiceActivationEnabled);
    console.log('ðŸ“ž Phone tone enabled:', phoneToneEnabled);
    console.log('ðŸ“ž Has user gesture:', hasUserGesture);
    console.log('ðŸŽ¨ Waiting for voice:', isWaitingForVoice);
    console.log('ðŸŽ¨ Exhibition interface URL:', window.location.href);
    console.log('ðŸŽ¨ URL params:', new URLSearchParams(window.location.search).toString());
    
    // FORCE START simple tone on mount
    console.log('ðŸ“ž FORCE STARTING simple tone on mount...');
    if (!simpleToneRef.current) {
      simpleToneRef.current = new SimplePhoneTone(0.05);
    }
    simpleToneRef.current.start().catch((err) => {
      console.log('ðŸ“ž Mount tone start failed:', err.message);
      if (err.message.includes('AudioContext') || err.message.includes('user gesture') || err.message.includes('not allowed to start')) {
        setShowAudioEnableOverlay(true);
      }
    });
  }, []);

  // Simple tone auto-start (separate effect with no dependencies)
  useEffect(() => {
    console.log('ðŸ“ž Auto-start effect running...');
    const timer = setTimeout(() => {
      console.log('ðŸ“ž AUTO-STARTING simple tone after 1 second...');
      if (!simpleToneRef.current) {
        simpleToneRef.current = new SimplePhoneTone(0.05);
      }
      simpleToneRef.current.start().catch((err) => {
        console.log('ðŸ“ž Auto-start failed:', err.message);
        if (err.message.includes('AudioContext') || err.message.includes('user gesture') || err.message.includes('not allowed to start')) {
          setShowAudioEnableOverlay(true);
        }
      });
    }, 1000);
    
    return () => clearTimeout(timer);
  }, []);

  // Simple tone management
  useEffect(() => {
    console.log('ðŸ“ž Simple tone effect:', { phoneToneEnabled, isWaitingForVoice, isCallActive });
    
    // SIMPLIFIED: Only play tone when waiting for voice AND tone is enabled
    if (phoneToneEnabled && isWaitingForVoice && !isCallActive) {
      console.log('ðŸ“ž Starting simple tone...');
      if (!simpleToneRef.current) {
        simpleToneRef.current = new SimplePhoneTone(0.05); // Quiet volume
      }
      simpleToneRef.current.start().catch((err) => {
        console.log('ðŸ“ž Simple tone start failed (will retry on gesture):', err.message);
      });
    } else if (simpleToneRef.current) {
      console.log('ðŸ“ž Stopping simple tone...');
      simpleToneRef.current.stop();
    }
  }, [phoneToneEnabled, isWaitingForVoice, isCallActive]);

  // Auto-scroll transcript
  useEffect(() => {
    if (transcriptContainerRef.current) {
      transcriptContainerRef.current.scrollTop = transcriptContainerRef.current.scrollHeight;
    }
  }, [callTranscript]);

  // Session timeout management
  const resetSessionTimeout = useCallback(() => {
    setLastActivityTime(Date.now());
    
    if (sessionTimeoutId) {
      clearTimeout(sessionTimeoutId);
    }
    
    if (isCallActive) {
      const newTimeoutId = window.setTimeout(() => {
        console.log('â° Session timeout - ending call');
        handleEndCallButtonClick();
      }, sessionTimeout);
      
      setSessionTimeoutId(newTimeoutId);
    }
  }, [sessionTimeoutId, isCallActive, sessionTimeout]);

  // User silence timeout management (ends call naturally after user stops speaking)
  const resetUserSilenceTimeout = useCallback((forceActive = false) => {
    const effectiveIsCallActive = forceActive || isCallActive;
    console.log('ðŸ”„ Resetting user silence timeout...', { userSilenceTimeout, isCallActive: effectiveIsCallActive, forceActive, userHasResponded });
    setLastUserTranscriptTime(Date.now());
    
    if (userSilenceTimeoutId) {
      console.log('â¹ï¸ Clearing existing user silence timeout');
      clearTimeout(userSilenceTimeoutId);
    }
    
    // Only start silence timer if call is active AND user has responded to Lars at least once
    if (effectiveIsCallActive && userSilenceTimeout && userHasResponded) {
      console.log(`â° Starting true silence detection: ${userSilenceTimeout}ms`);
      
      // Use polling to check for true silence (no speech from anyone)
      const checkForSilence = () => {
        const now = Date.now();
        const timeSinceLastSpeech = now - lastAnySpeechTime;
        console.log(`ðŸ” SILENCE CHECK: ${timeSinceLastSpeech}ms since last speech (threshold: ${userSilenceTimeout}ms)`);
        console.log(`ðŸ” Times: now=${now}, lastSpeech=${lastAnySpeechTime}, diff=${timeSinceLastSpeech}`);
        
        if (timeSinceLastSpeech >= userSilenceTimeout) {
          console.log(`â° SILENCE TIMEOUT TRIGGERED! ${timeSinceLastSpeech}ms >= ${userSilenceTimeout}ms`);
          console.log('ðŸ¤ True silence detected - ending call naturally');
          handleSilenceTimeout();
        } else {
          // Schedule next check
          const remainingTime = userSilenceTimeout - timeSinceLastSpeech;
          const nextCheckTime = Math.min(remainingTime, 1000); // Check every 1s or when timeout should occur
          const timeoutId = window.setTimeout(checkForSilence, nextCheckTime);
          setUserSilenceTimeoutId(timeoutId);
        }
      };
      
      // Start checking after a small delay
      const timeoutId = window.setTimeout(checkForSilence, 1000);
      setUserSilenceTimeoutId(timeoutId);
      
    } else {
      console.log('âŒ Not setting silence timeout:', { isCallActive: effectiveIsCallActive, userSilenceTimeout, userHasResponded });
    }
    
    // Helper function to handle silence timeout
    const handleSilenceTimeout = async () => {
      try {
        setAgentStatus('Ending call due to silence...');
        
        // Clear all timeouts
        if (userSilenceTimeoutId) {
          clearTimeout(userSilenceTimeoutId);
          setUserSilenceTimeoutId(null);
        }
        
        // End call and return to waiting
        console.log('ðŸ”š CALLING endCall() function...');
        await endCall();
        console.log('ðŸ”š endCall() completed, session should disconnect now');
        
        // Reset all state to waiting state manually (can't reference returnToWaitingState due to hoisting)
        console.log('ðŸ”„ SILENCE TRIGGERED: Manually resetting state to waiting mode');
        setIsCallActive(false);
        setIsWaitingForVoice(true);
        setPhoneToneEnabled(true);
        setVoiceActivationEnabled(true);
        setCallTranscript([]);
        setCurrentConversation(null);
        setUltravoxCallId(null);
        conversationRef.current = null;
        setAgentStatus('ready');
        setUserHasResponded(false);
        setLastAnySpeechTime(Date.now());
        
        // Force restart phone tone
        setTimeout(() => {
          console.log('ðŸ“ž Force restarting phone tone after silence timeout...');
          if (!simpleToneRef.current) {
            simpleToneRef.current = new SimplePhoneTone(0.05);
          }
          simpleToneRef.current.start().catch((err) => {
            console.log('ðŸ“ž Force restart failed:', err.message);
            if (err.message.includes('AudioContext') || err.message.includes('user gesture') || err.message.includes('not allowed to start')) {
              setShowAudioEnableOverlay(true);
            }
          });
        }, 100);
        
        onSessionEnd?.();
        console.log('âœ… SILENCE TIMEOUT COMPLETE: Exhibition session ended due to true silence, app ready for next user');
        
      } catch (error) {
        console.error('âŒ Error ending call due to silence:', error);
      }
    };
  }, [userSilenceTimeoutId, isCallActive, userSilenceTimeout, userHasResponded, lastAnySpeechTime, onSessionEnd]);

  // Clear user silence timeout
  const clearUserSilenceTimeout = useCallback(() => {
    if (userSilenceTimeoutId) {
      clearTimeout(userSilenceTimeoutId);
      setUserSilenceTimeoutId(null);
    }
  }, [userSilenceTimeoutId]);


  // Unified return to waiting state function (voice-only exhibition)
  const returnToWaitingState = useCallback(async (reason: string = 'unknown') => {
    console.log(`ðŸ”„ RETURN TO WAITING STATE - Reason: ${reason}`);
    console.log(`ðŸ”„ Current state: isCallActive=${isCallActive}, isWaitingForVoice=${isWaitingForVoice}`);
    console.log(`ðŸ”„ Conversation data: currentConversation=${currentConversation?.id}, ultravoxCallId=${ultravoxCallId}`);
    
    try {
      // Clear session timeout
      if (sessionTimeoutId) {
        clearTimeout(sessionTimeoutId);
        setSessionTimeoutId(null);
      }
      
      // Clear user silence timeout
      clearUserSilenceTimeout();
      
      // SIMPLE TRANSCRIPT SAVE - Just fetch from Ultravox and save to last conversation
      setTimeout(async () => {
        try {
          console.log('ðŸ’¾ Saving transcript from Ultravox...');
          const response = await fetch('/api/save-last-transcript', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' }
          });
          
          if (response.ok) {
            const result = await response.json();
            console.log('âœ… Transcript saved:', result);
          } else {
            console.warn('âš ï¸ Transcript save failed:', response.status);
          }
        } catch (error) {
          console.error('âŒ Failed to save transcript:', error);
        }
      }, 2000); // Wait 2 seconds for call to fully end
      
      // Reset all state to waiting state
      setIsCallActive(false);
      setIsWaitingForVoice(true);
      setPhoneToneEnabled(true); // Restart phone tone
      setVoiceActivationEnabled(true);
      setCallTranscript([]);
      setCurrentConversation(null);
      setUltravoxCallId(null);
      conversationRef.current = null;
      setAgentStatus('ready');
      setUserHasResponded(false); // Reset user response tracking for next call
      setLastAnySpeechTime(Date.now()); // Reset speech tracking for next call
      setConversationEnding(false); // Reset conversation ending flag
      
      // Let main tone effect handle restart based on state changes
      console.log('ðŸ“ž State reset complete - main tone effect will handle restart');
      
      onSessionEnd?.();
      console.log('âœ… RETURN TO WAITING COMPLETE - Exhibition session ended, app ready for next user');
      console.log(`âœ… Final state: isCallActive=false, isWaitingForVoice=true, phoneToneEnabled=true, agentStatus=ready`);
    } catch (error) {
      console.error('âŒ Error returning to waiting state:', error);
      setAgentStatus(`Error: ${error instanceof Error ? error.message : String(error)}`);
    }
  }, [sessionTimeoutId, currentConversation, ultravoxCallId, callTranscript, onSessionEnd, clearUserSilenceTimeout, hasUserGesture]);

  // Handle status changes (voice-only exhibition)
  const handleStatusChange = useCallback(async (status: UltravoxSessionStatus | string | undefined) => {
    console.log('ðŸ“¡ Ultravox status change:', status);
    
    if (status) {
      setAgentStatus(status);
      resetSessionTimeout(); // Reset timeout on any activity
      
      // CRITICAL: Detect when Ultravox indicates call has ended naturally
      if (status === 'disconnected' || status === 'disconnecting') {
        console.log(`ðŸ”Œ Ultravox session ${status} - forcing return to waiting state`);
        console.log(`ðŸ”Œ Current states: isCallActive=${isCallActive}, isWaitingForVoice=${isWaitingForVoice}`);
        
        // FORCE immediate state reset regardless of current state to fix race condition
        console.log('ðŸ”Œ Forcing immediate state reset to prevent navigation failure');
        setIsCallActive(false);
        setIsWaitingForVoice(true);
        setPhoneToneEnabled(true);
        setAgentStatus('ready');
        
        // Then call the full cleanup function (non-blocking)
        console.log('ðŸ”Œ Calling full cleanup function (non-blocking)');
        returnToWaitingState('ultravox session ended'); // Remove await to prevent blocking
        return;
      }
    } else {
      setAgentStatus('ready');
    }
  }, [resetSessionTimeout, isCallActive, isWaitingForVoice, returnToWaitingState]);

  // Handle transcript changes with agent detection
  const handleTranscriptChange = useCallback(async (transcripts: Transcript[] | undefined) => {
    if (transcripts) {
      setCallTranscript([...transcripts]);
      resetSessionTimeout(); // Reset timeout on transcript activity
      
      // Update last speech time for anyone speaking
      const currentTime = Date.now();
      setLastAnySpeechTime(currentTime);
      console.log(`ðŸ”Š SPEECH DETECTED: Updated lastAnySpeechTime to ${currentTime}`);
      
      // User silence timeout logic: Reset when user speaks, let it continue when only agent speaks
      const userTranscripts = transcripts.filter(t => t.speaker === 'user');
      const agentTranscripts = transcripts.filter(t => t.speaker === 'agent');
      
      console.log('ðŸ“ Transcript update:', { 
        totalTranscripts: transcripts.length, 
        userCount: userTranscripts.length, 
        agentCount: agentTranscripts.length,
        lastAnySpeech: new Date(currentTime).toLocaleTimeString()
      });
      
      if (userTranscripts.length > 0) {
        const latestUserTranscript = userTranscripts[userTranscripts.length - 1];
        console.log('ðŸ‘¤ Latest user transcript:', latestUserTranscript.text);
        // Reset user silence timeout when user actually speaks
        if (latestUserTranscript && latestUserTranscript.text.trim().length > 0) {
          console.log('ðŸ‘¤ User spoke - resetting silence timeout');
          
          // Mark that user has responded (enables silence detection for future)
          if (!userHasResponded) {
            console.log('ðŸŽ¯ First user response detected - enabling silence detection');
            setUserHasResponded(true);
          }
          
          resetUserSilenceTimeout();
        }
      } else if (agentTranscripts.length > 0) {
        // Agent is speaking - this resets the silence timer
        const latestAgentTranscript = agentTranscripts[agentTranscripts.length - 1];
        console.log('ðŸ¤– Agent speaking, resetting silence timer...', latestAgentTranscript.text.substring(0, 50));
      } else {
        console.log('ðŸ”‡ No transcripts yet...');
      }
      
      // Agent detection logic (same as main app)
      transcripts.forEach((transcript) => {
        const text = transcript.text;
        
        if (text.includes('[AGENT: LARS]')) {
          setCurrentAgent('lars');
          setCurrentVoiceId('876ac038-08f0-4485-8b20-02b42bcf3416');
        } else if (text.includes('[AGENT: WIKTORIA]')) {
          setCurrentAgent('wiktoria');
          setCurrentVoiceId('2e40bf21-8c36-45db-a408-5a3fc8d833db');
        }
      });
      
      // Content-based agent detection (reuse agentTranscripts from above)
      if (agentTranscripts.length > 0) {
        const latestAgentTranscript = agentTranscripts[agentTranscripts.length - 1];
        const text = latestAgentTranscript.text.toLowerCase();
        
        // Stop phone tone when agent starts speaking (throttled logging)
        if (phoneToneEnabled) {
          console.log('ðŸ¤– Agent speaking detected - stopping phone tone');
        }
        if (simpleToneRef.current) {
          simpleToneRef.current.stop();
        }
        setPhoneToneEnabled(false);
        
        // Lars detection patterns
        if (text.includes('synthetic party') || text.includes('leader lars') || 
            text.includes('*coughs*') || text.includes('bureaucracy')) {
          setCurrentAgent('lars');
          setCurrentVoiceId('876ac038-08f0-4485-8b20-02b42bcf3416');
        }
        // Wiktoria detection patterns
        else if (text.includes('jestem wiktoria') || text.includes('president') ||
                 text.includes('*smirks*') || text.includes('prezydent')) {
          setCurrentAgent('wiktoria');
          setCurrentVoiceId('2e40bf21-8c36-45db-a408-5a3fc8d833db');
        }
        
        // Check if conversation ended naturally with goodbye message
        const endPattern = /dziÄ™kujÄ™ za rozmowÄ™.*dobiegÅ‚a koÅ„ca.*do zobaczenia/i;
        if (endPattern.test(latestAgentTranscript.text) && !conversationEnding) {
          console.log('ðŸ‘‹ End message detected - conversation finished naturally');
          setConversationEnding(true); // Prevent duplicate triggers
          // Give time for message to complete, then return to waiting
          setTimeout(async () => {
            console.log('ðŸ‘‹ Triggering return to waiting after end message');
            await returnToWaitingState('natural conversation end');
          }, 2000);
        }
      }
    }
  }, [resetSessionTimeout, resetUserSilenceTimeout, returnToWaitingState, phoneToneEnabled, conversationEnding]);

  // Handle debug messages
  const handleDebugMessage = useCallback((debugMessage: UltravoxExperimentalMessageEvent) => {
    resetSessionTimeout(); // Reset timeout on debug activity
    
    // Agent detection from debug messages (same logic as main app)
    const messageText = JSON.stringify(debugMessage);
    
    if (messageText.includes('[AGENT: LARS]')) {
      setCurrentAgent('lars');
      setCurrentVoiceId('876ac038-08f0-4485-8b20-02b42bcf3416');
    } else if (messageText.includes('[AGENT: WIKTORIA]')) {
      setCurrentAgent('wiktoria');
      setCurrentVoiceId('2e40bf21-8c36-45db-a408-5a3fc8d833db');
    }
  }, [resetSessionTimeout]);

  // Start call when voice is activated
  const handleVoiceActivation = useCallback(async () => {
    if (isCallActive) {
      return; // Already in call
    }
    
    try {
      console.log('ðŸš€ Voice activation triggered - starting call');
      setIsWaitingForVoice(false);
      
      // DON'T stop tone yet - let it continue until Lars speaks
      console.log('ðŸ“ž Call starting - tone will continue until Lars responds');
      
      setAgentStatus('Starting voice call...');
      setCallTranscript([]);
      setConversationEnding(false); // Reset conversation ending flag for new call
      
      // Generate call key
      const newKey = `exhibition-${Date.now()}`;
      
      // Setup call config
      let callConfig: CallConfig = {
        systemPrompt: demoConfig.callConfig.systemPrompt,
        model: demoConfig.callConfig.model,
        languageHint: demoConfig.callConfig.languageHint,
        voice: demoConfig.callConfig.voice,
        temperature: demoConfig.callConfig.temperature,
        maxDuration: demoConfig.callConfig.maxDuration,
        timeExceededMessage: demoConfig.callConfig.timeExceededMessage
      };

      const paramOverride: { [key: string]: any } = {
        "callId": newKey
      };

      let cpTool: SelectedTool | undefined = demoConfig?.callConfig?.selectedTools?.find(tool => tool.toolName === "createProfile");
      if (cpTool) {
        cpTool.parameterOverrides = paramOverride;
      }
      callConfig.selectedTools = demoConfig.callConfig.selectedTools;

      // Create database conversation
      try {
        const conversation = await saveConversation({
          ultravox_call_id: newKey,
          user_name: 'Exhibition Visitor',
          topic: 'Art Exhibition Interaction'
        });
        
        conversationRef.current = conversation;
        setCurrentConversation(conversation);
        console.log('ðŸ’¾ EXHIBITION CONVERSATION CREATED:', conversation.id);
        console.log('ðŸ’¾ CONVERSATION OBJECT:', conversation);
      } catch (error) {
        console.error('âŒ Failed to create exhibition conversation:', error);
      }

      // Start the actual call
      const callData = await startCall({
        onStatusChange: handleStatusChange,
        onTranscriptChange: handleTranscriptChange,
        onDebugMessage: handleDebugMessage
      }, callConfig, showDebugInfo);

      setUltravoxCallId(callData.callId);
      console.log(`ðŸ”— ULTRAVOX CALL ID SET: ${callData.callId}`);
      setIsCallActive(true);
      resetSessionTimeout(); // Start session timeout
      // DO NOT start user silence timeout yet - wait for first user response to Lars
      onSessionStart?.();
      
      console.log('âœ… Exhibition call started successfully');
    } catch (error) {
      console.error('âŒ Failed to start exhibition call:', error);
      setAgentStatus(`Error: ${error instanceof Error ? error.message : String(error)}`);
      
      // Resume waiting state on error
      setIsWaitingForVoice(true);
      setPhoneToneEnabled(true);
    }
  }, [isCallActive, handleStatusChange, handleTranscriptChange, handleDebugMessage, resetSessionTimeout, resetUserSilenceTimeout, onSessionStart, showDebugInfo]);

  // End call function (session timeout or user silence - calls endCall then returns to waiting)
  const handleEndCallButtonClick = useCallback(async () => {
    try {
      console.log('â° Ending call (timeout or silence detected)');
      setAgentStatus('Ending call...');
      
      // Clear all timeouts
      clearUserSilenceTimeout();
      
      await endCall();
      await returnToWaitingState('call ended');
    } catch (error) {
      console.error('âŒ Error ending exhibition call:', error);
      setAgentStatus(`Error ending: ${error instanceof Error ? error.message : String(error)}`);
    }
  }, [returnToWaitingState, clearUserSilenceTimeout]);

  // Voice activity handlers
  const handleVoiceStart = useCallback(() => {
    console.log('ðŸŽ¤ Voice activity detected in exhibition');
    resetSessionTimeout();
    
    // Reset user silence timeout when user speaks
    if (isCallActive) {
      console.log('ðŸ‘¤ User voice activity - resetting silence timeout');
      resetUserSilenceTimeout();
    }
    
    // Capture user gesture for AudioContext - create/resume tone
    if (!hasUserGesture) {
      console.log('ðŸ“ž User gesture detected - initializing AudioContext for tone');
      setHasUserGesture(true);
      
      // Initialize SimplePhoneTone with user gesture
      if (!simpleToneRef.current) {
        simpleToneRef.current = new SimplePhoneTone(0.05);
      }
      
      // AudioContext initialized, let main effect handle tone management
      console.log('ðŸ“ž AudioContext ready - tone management handled by main effect');
    } else {
      // User is speaking - stop tone if playing
      if (simpleToneRef.current) {
        console.log('ðŸ“ž User speaking - stopping tone');
        simpleToneRef.current.stop();
      }
    }
  }, [resetSessionTimeout, resetUserSilenceTimeout, isCallActive, hasUserGesture, phoneToneEnabled, isWaitingForVoice]);

  const handleVoiceEnd = useCallback(() => {
    console.log('ðŸŽ¤ Voice activity ended in exhibition');
  }, []);

  const handleVoiceDetected = useCallback(() => {
    console.log('ðŸŽ¤ Voice detected - stopping tone immediately');
    // Stop tone when voice is detected
    if (simpleToneRef.current) {
      simpleToneRef.current.stop();
    }
    setPhoneToneEnabled(false);
  }, []);

  // No button handlers needed - pure voice activation

  // Handle click to enable audio
  const handleEnableAudio = useCallback(async () => {
    console.log('ðŸ“ž User clicked to enable audio');
    setHasUserGesture(true);
    setShowAudioEnableOverlay(false);
    
    if (!simpleToneRef.current) {
      simpleToneRef.current = new SimplePhoneTone(0.05);
    }
    
    try {
      await simpleToneRef.current.start();
      console.log('âœ… Audio enabled successfully after user click');
    } catch (error) {
      console.error('âŒ Audio still failed after user click:', error);
    }
  }, []);

  // Get current agent display name
  const getCurrentAgentLabel = () => {
    return currentAgent === 'lars' ? 'LEADER LARS' : 'WIKTORIA CUKT 2.0';
  };

  return (
    <div className="exhibition-interface h-screen w-full bg-black text-white flex flex-col">
      {/* Audio Enable Overlay */}
      {showAudioEnableOverlay && (
        <div className="fixed inset-0 bg-black bg-opacity-90 flex items-center justify-center z-50">
          <div className="text-center p-8">
            <div className="text-6xl mb-4">ðŸ”Š</div>
            <h2 className="text-3xl font-bold mb-4">Enable Audio</h2>
            <p className="text-xl mb-6 text-gray-300">Click to activate exhibition sound</p>
            <button
              onClick={handleEnableAudio}
              className="bg-white text-black px-8 py-4 text-xl font-bold rounded-lg hover:bg-gray-200 transition-colors"
            >
              Enable Sound
            </button>
          </div>
        </div>
      )}
      
      {/* Minimal Header */}
      <div className="p-6 text-center">
        <h1 className="text-4xl font-bold mb-2">AI POLITICAL PERFORMANCE</h1>
        <div className="text-lg text-gray-300">
          {isWaitingForVoice 
            ? 'MÃ³w â€¢ Start to speak â€¢ Nie bÃ³j siÄ™' 
            : isCallActive 
              ? getCurrentAgentLabel()
              : 'System Ready'
          }
        </div>
      </div>

      {/* Main Content Area */}
      <div className="flex-1 flex flex-col justify-center px-8">
        {isWaitingForVoice ? (
          /* Waiting for Voice State */
          <div className="text-center space-y-8">
            <div className="text-6xl">ðŸŽ¤</div>
            <div className="text-2xl font-light">
              Start speaking to begin your conversation with Lars and Wiktoria
            </div>
            
            {/* SIMPLE TONE BUTTON */}
            <button 
              onClick={() => {
                console.log('ðŸ”¥ SIMPLE TONE START');
                if (!simpleToneRef.current) {
                  simpleToneRef.current = new SimplePhoneTone(0.1);
                }
                simpleToneRef.current.start().catch(console.error);
              }}
              className="px-4 py-2 bg-green-600 text-white rounded"
            >
              ðŸ”¥ SIMPLE TONE
            </button>

            {/* EMERGENCY MANUAL START BUTTON */}
            <button 
              onClick={async () => {
                console.log('ðŸ”¥ DIRECT TONE TEST');
                try {
                  // Create AudioContext directly
                  const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
                  console.log('ðŸ”¥ AudioContext state:', audioContext.state);
                  
                  if (audioContext.state === 'suspended') {
                    await audioContext.resume();
                    console.log('ðŸ”¥ AudioContext resumed, new state:', audioContext.state);
                  }
                  
                  // Create oscillators for dial tone
                  const gainNode = audioContext.createGain();
                  gainNode.connect(audioContext.destination);
                  gainNode.gain.value = 0.1;
                  
                  const osc1 = audioContext.createOscillator();
                  const osc2 = audioContext.createOscillator();
                  
                  osc1.frequency.setValueAtTime(350, audioContext.currentTime);
                  osc2.frequency.setValueAtTime(440, audioContext.currentTime);
                  
                  osc1.connect(gainNode);
                  osc2.connect(gainNode);
                  
                  osc1.start();
                  osc2.start();
                  
                  console.log('ðŸ”¥ DIAL TONE SHOULD BE PLAYING NOW!');
                  
                  // Stop after 3 seconds
                  setTimeout(() => {
                    osc1.stop();
                    osc2.stop();
                    console.log('ðŸ”¥ Stopped dial tone');
                  }, 3000);
                  
                } catch (error) {
                  console.error('ðŸ”¥ Direct tone test failed:', error);
                }
              }}
              className="px-4 py-2 bg-red-600 text-white rounded"
            >
              ðŸ”¥ DIRECT TONE TEST
            </button>
            
            {/* Voice Activation Component */}
            <VoiceActivation
              enabled={voiceActivationEnabled}
              isCallActive={isCallActive}
              onVoiceActivation={handleVoiceActivation}
              onVoiceStart={handleVoiceStart}
              onVoiceEnd={handleVoiceEnd}
              onVoiceDetected={handleVoiceDetected}
              showVisualFeedback={true}
            />
            
            {/* TEMP DISABLED: Phone Line Tone Component 
            <PhoneTonePlayer
              ref={phoneToneRef}
              enabled={phoneToneEnabled && isWaitingForVoice}
              toneType="dial"
              showControls={showDebugInfo}
              onStart={() => console.log('ðŸ“ž Phone tone started')}
              onStop={() => console.log('ðŸ“ž Phone tone stopped')}
              onError={(error) => console.error('ðŸ“ž Phone tone error:', error)}
            />
            */}
          </div>
        ) : isCallActive ? (
          /* Active Call State */
          <div className="space-y-6">
            {/* Agent Status */}
            <div className="text-center">
              <div className="text-2xl font-bold text-red-400 mb-2">
                {getCurrentAgentLabel()}
              </div>
              <div className="text-sm text-gray-400">
                {agentStatus}
              </div>
            </div>

            {/* Transcript Display */}
            <div 
              ref={transcriptContainerRef}
              className="h-64 overflow-y-auto bg-gray-900 rounded-lg p-4 space-y-3"
            >
              {callTranscript && callTranscript.map((transcript, index) => (
                <div key={index} className="text-sm">
                  {transcript.speaker === 'user' ? (
                    <div className="text-blue-400">
                      <strong>YOU:</strong> {transcript.text}
                    </div>
                  ) : (
                    <div className="text-white">
                      <strong className="text-red-400">
                        {currentAgent === 'lars' ? 'LARS:' : 'WIKTORIA:'}
                      </strong> {transcript.text}
                    </div>
                  )}
                </div>
              ))}
            </div>


            {/* Session Info */}
            {showDebugInfo && (
              <div className="text-center text-xs text-gray-500">
                Session active â€¢ Auto-timeout in {Math.round(sessionTimeout / 1000)}s
              </div>
            )}
          </div>
        ) : (
          /* System Ready State */
          <div className="text-center">
            <div className="text-xl text-gray-400">System Ready</div>
          </div>
        )}
      </div>

      {/* Footer with Manual Controls (Development Only) */}
      {showDebugInfo && (
        <div className="p-4 bg-gray-900 text-xs">
          <div className="flex justify-between items-center">
            <div>Mode: {modeConfig.mode} | Voice: {voiceActivationEnabled ? 'ON' : 'OFF'} | Phone Tone: {phoneToneEnabled ? 'ON' : 'OFF'}</div>
            <div className="space-x-2">
              <button
                onClick={() => {
                  const logs = localStorage.getItem('exhibition_logs');
                  if (logs) {
                    const blob = new Blob([JSON.parse(logs).join('\n')], { type: 'text/plain' });
                    const url = URL.createObjectURL(blob);
                    const a = document.createElement('a');
                    a.href = url;
                    a.download = 'exhibition_logs.txt';
                    a.click();
                    URL.revokeObjectURL(url);
                  } else {
                    alert('No logs available');
                  }
                }}
                className="px-3 py-1 bg-purple-600 rounded hover:bg-purple-700"
              >
                ðŸ“‹ Download Logs
              </button>
              
              {isCallActive ? (
                <button
                  onClick={handleEndCallButtonClick}
                  className="px-3 py-1 bg-red-600 rounded hover:bg-red-700"
                >
                  End Call
                </button>
              ) : (
                <button
                  onClick={handleVoiceActivation}
                  className="px-3 py-1 bg-green-600 rounded hover:bg-green-700"
                >
                  Manual Start
                </button>
              )}
            </div>
          </div>
        </div>
      )}
      
      {/* Always visible log download button */}
      <div className="fixed bottom-4 right-4">
        <button
          onClick={() => {
            const logs = localStorage.getItem('exhibition_logs');
            if (logs) {
              const blob = new Blob([JSON.parse(logs).join('\n')], { type: 'text/plain' });
              const url = URL.createObjectURL(blob);
              const a = document.createElement('a');
              a.href = url;
              a.download = 'exhibition_logs.txt';
              a.click();
              URL.revokeObjectURL(url);
            } else {
              alert('No logs available');
            }
          }}
          className="px-3 py-1 bg-purple-600 text-white rounded hover:bg-purple-700 text-xs"
        >
          ðŸ“‹ Logs
        </button>
      </div>
    </div>
  );
}